{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Linear Classifiers</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cPickle as pickle\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "%pylab inline\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "import random\n",
    "from IPython import display\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">2D datasets</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\n",
    "X += np.random.random(X.shape)\n",
    "\n",
    "datasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 14, 4\n",
    "pl = plt.subplots(1, len(datasets), sharex='col', sharey='row')\n",
    "for i, (X, y) in enumerate(datasets):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    pl[1][i].scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">CIFAR dataset</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cifar10_dir = './data/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (14.0, 5.0) \n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 4\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        pylab.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        pylab.imshow(X_train[idx].astype('uint8'))\n",
    "        pylab.axis('off')\n",
    "        if i == 0:\n",
    "            pylab.title(cls)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $$y_{predict}(x) = sign(<w, x>)~~~~~~margin(x, y) = y \\cdot sign(<w, x>)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простой пример классфикации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, w, y_true = [0.01, 1], [-0.1, 0.15], -1\n",
    "print 'margin', y_true*np.dot(x, w), 'label', np.sign(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (12.0, 5.0) \n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, 20), np.linspace(0, 1, 20))\n",
    "y_pred = w[0]*xx + w[1]*yy\n",
    "\n",
    "pl = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "pl[1][0].set_xlim([0, 1])\n",
    "pl[1][0].set_ylim([0, 1])\n",
    "\n",
    "cax = pl[1][0].pcolormesh(xx, yy, y_pred, cmap='rainbow')\n",
    "pl[1][0].scatter(xx, yy, c=y_pred, alpha=0.8, cmap='rainbow')\n",
    "pylab.colorbar(cax, ticks=[-1, 0, 1])\n",
    "\n",
    "pl[1][1].set_xlim([0, 1])\n",
    "pl[1][1].set_ylim([0, 1])\n",
    "\n",
    "cax = pl[1][1].pcolormesh(xx, yy, np.sign(y_pred), cmap='rainbow')\n",
    "pl[1][1].scatter(xx, yy, c=np.sign(y_pred), alpha=0.8, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">General optimization problem for Linear Classifiers</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $$L = \\frac{1}{n} \\sum_i^n L_i(x_i, y_i^{real}, y_i^{predict}(w); w) \\rightarrow \\min_w ~~~~~~ y^{predict}(x) = <x, w>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loss functions:\n",
    "    - Hinge Loss \n",
    "## $$L_i(x, y; w) = max(0, 1 - y\\cdot<w, x>)$$\n",
    "    - Loistic Loss \n",
    "## $$L_i(x, y; w) = log(1 + e^{-y\\cdot<w, x>})$$\n",
    "    - Squared Loss\n",
    "## $$L_i(x, y; w) = log(1 - y\\cdot<w, x>)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (7.0, 7.0) \n",
    "x = np.linspace(-1, 2.5)\n",
    "pylab.plot(x, map(lambda m: np.max([0, 1 - m]), x), label='hinge')\n",
    "pylab.plot(x, map(lambda m: np.log(1 + e**(-m)), x), label='logistic')\n",
    "pylab.plot(x, map(lambda m: (1 - m)**2, x), label='squared')\n",
    "pylab.ylabel('Loss')\n",
    "pylab.xlabel('Margin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нелинейная линейная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    X_ = np.zeros((X.shape[0], 6))\n",
    "    X_[:,0:2] = X\n",
    "    X_[:,2:4] = X**2\n",
    "    X_[:,4] = X[:,0] * X[:,1]\n",
    "    X_[:,5] = 1;\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Как вычислять градиент</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матричные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найти производные приведенных функций по w, в матричной форме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $$f(w) = <x, w>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $$f(w) = \\sum_i log(1-e^{-y_i <x_i, w>})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Численное дифиренцирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display.Image('img/finnit_diff.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://en.wikipedia.org/wiki/Numerical_differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Символьное дифиренцирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "derivative = T.grad(scalar_squared, my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3)\n",
    "x_squared = map(fun,x)\n",
    "x_squared_der = map(grad,x)\n",
    "\n",
    "pylab.plot(x, x_squared,label=\"x^2\")\n",
    "pylab.plot(x, x_squared_der, label=\"derivative\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(X, w):\n",
    "    return np.sign(1.0 / (1.0 + np.exp(X.dot(w))) - 0.5)\n",
    "\n",
    "w = T.vector(\"w\", dtype='float64')\n",
    "input_X = T.matrix(\"X\", dtype='float64')\n",
    "input_y = T.vector(\"y\", dtype='float64')\n",
    "\n",
    "\n",
    "margin = -input_y*input_X.dot(w)\n",
    "\n",
    "loss = T.log(1+T.exp(-margin)).mean()\n",
    "grad = T.grad(loss, w)\n",
    "hess = T.hessian(loss, w)\n",
    "\n",
    "loss_function = theano.function([input_X, input_y, w], loss)\n",
    "grad_function = theano.function([input_X, input_y, w], grad)\n",
    "hess_function = theano.function([input_X, input_y, w], hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 align=\"center\">Выпуклая задача</h2> \n",
    "    - Что такое выпуклая функция\n",
    "    - Сколько оптимумов имеет выпуклая задача?\n",
    "    - Как понять что задача выпуклая?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display.Image('img/conv.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 align=\"center\">Оптимизация</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize(X, y, w, loss, n_iter, h=0.01):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    plt.clf()\n",
    "    Z = classify(expand(np.c_[xx.ravel(), yy.ravel()]), w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.contourf(xx, yy, Z, cmap='rainbow', alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(loss)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viz_opt(func, gradf, hessf, X, y, get_direction, w0=None, batch_size=None, alpha=None, mu=0.9, n_iter=10):\n",
    "    a = None\n",
    "    loss1 = np.zeros(n_iter)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    ind = np.arange(X.shape[0])\n",
    "    \n",
    "    X_, y_ = X, y\n",
    "    w, d = np.zeros(X_.shape[1]), np.zeros(X_.shape[1])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        if batch_size:\n",
    "            ind = random.sample(range(X.shape[0]), batch_size)\n",
    "            X_, y_ = X[ind], y[ind]\n",
    "            \n",
    "        loss1[i] += loss_function(X_, y_, w)\n",
    "        visualize(X_, y_, w, loss1, n_iter)\n",
    "        \n",
    "        d = get_direction(X_, y_, w, gradf, hessf, d)\n",
    "        \n",
    "        if not alpha:\n",
    "            f = lambda w: loss_function(X_, y_, w)\n",
    "            g = lambda w: grad_function(X_, y_, w)\n",
    "            a = optimize.line_search(f, g, w, d)[0]\n",
    "            \n",
    "        if not a:\n",
    "            a = 1e-4\n",
    "            \n",
    "        w = w + a*d\n",
    "        \n",
    "    visualize(X, y, w, loss1, n_iter)\n",
    "    \n",
    "    q = plt.clf()\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Градинтный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$x_{k+1} = x_k - \\Delta f(x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска обладает линейной скоростью сходимости. \n",
    "Что достачтоно меленно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    return X\n",
    "\n",
    "def gd(X, y, w, gradf, hessf, dold):\n",
    "    return -grad_function(X, y, w)\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, gd) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    X_ = np.zeros((X.shape[0], 6))\n",
    "    X_[:,0:2] = X\n",
    "    X_[:,2:4] = X**2\n",
    "    X_[:,4] = X[:,0] * X[:,1]\n",
    "    X_[:,5] = 1;\n",
    "    return X_\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, gd) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нютон (HF, BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$x_{k+1} = x_k - \\Delta^2 f(x_k) \\Delta f(x_k)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы второго порядка -- намного быстрее, но как правило дорогие тк требуют хранения гессиана.\n",
    "\n",
    "Некоторые методы второго порядка лишены жтого недостатка, при необходимсти используйте BFGS или HF Newton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nw(X, y, w, gradf, hessf, dold):\n",
    "    return -np.linalg.inv(hessf(X, y, w)).dot(grad_function(X, y, w))\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, nw) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стахастичиский град спуск, моментум, нестеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать если в функции большая сумма? давайте считать градиент только по случайной подвыборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$x_{k+1} = x_k -  \\Delta \\hat{f}(x_k)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gd(X, y, w, gradf, hessf, dold):\n",
    "    return -grad_function(X, y, w)\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, gd, batch_size=10) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$x_{k+1} = x_k - E \\Delta \\hat{f}(x_k)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def momentum(X, y, w, gradf, hessf, dold, mu=0.9):\n",
    "    return mu * dold - (1-mu) * grad_function(X, y, w)\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, momentum, batch_size=10) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$x_{k+1} = x_k - E \\Delta \\hat{f}(x_k-\\alpha E \\Delta \\hat{f}_{k-1})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nesterow(X, y, w, gradf, hessf, dold, mu=0.9, alpha=1e-3):\n",
    "    return mu * dold - (1-mu) * grad_function(X, y, w + alpha*dold)\n",
    "\n",
    "for X, y in datasets:\n",
    "    X, y = expand(X), -2*(y-0.5)\n",
    "    a = viz_opt(loss_function, grad_function, hess_function, X, y, nesterow, batch_size=10) \n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор шага "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Неточная одномерная оптимизация Wolfe conditions, константа наверняка плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $$f(\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k)\\leq f(\\mathbf{x}_k)+c_1\\alpha_k\\mathbf{p}_k^{\\mathrm T}\\nabla f(\\mathbf{x}_k)$$\n",
    "\n",
    "## $$\\mathbf{p}_k^{\\mathrm T}\\nabla f(\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k) \\geq c_2\\mathbf{p}_k^{\\mathrm T}\\nabla f(\\mathbf{x}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор начального приближения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Обычно случайный близкий к нулю вектор\n",
    "- Если задача не выпклая то лучше несколько случайных приближений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Multiclass Generalization</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display.Image('img/imagemap.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display.Image('img/pixelspace.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$ L_i = \\sum_{j, j \\neq y_i} max(0, w_j^tx - w_{y_i}^tx + 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display.Image('img/multisvm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=\"center\">Advantages and Disadvantages</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Достоинства:\n",
    "    - Быстрые\n",
    "    - Работают\n",
    "    - Интерпретируемы\n",
    "    - Применимы к большим данным\n",
    "    - Можно обучать онлайн\n",
    "Недостатки:\n",
    "    - Не всегда хороши"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> When are you should use linear models?</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
